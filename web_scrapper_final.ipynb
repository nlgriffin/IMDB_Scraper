{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Web Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "William Egan - wve204 | Tinatin Nikvashvili - tn709 | Nathan Griffin - nlg297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract urls from IMBD search\n",
    "[IMDB Search](https://www.imdb.com/search/title/)\n",
    "\n",
    "This project is motivated by the groups interest in movies and being able to pull some specific datasets for analysis on demand. While IMDB does have some datasets puiblicly available, they are small (at most 8 columns), and would require significant merging to obtain what we are going for. Further, there is no box office information in these datasets, and earnings is the most significant variable that we would like to analyze. Beyond insterest in this industry, this application is also motivated by the fact that web scraping is a valuable tool for every data scientist to have at their disposal. Any time there is a lack of data in a problem you are facing, there is potentially valuable information available somewhere on the internet, and being able to extract it, clean it, and test it's validity allows for solving problems that otherwise may seem unsolvable. \n",
    "\n",
    "The first step in this application is to take an input search (link above) and return a list of urls: one for each movie that satisfies those parameters. To implement this function, we will need the help of a few libraries: BeautifulSoup, urllib, and webbrowser (for debugging). The first thing to do is just take a look at what the search returns in the actual webpage. The things that jump out are: the total number of movies that match the search listed at the top as well as there being 50 movies per page. Therefore, we know two important variables right off the bat: how many urls we expect to return and how many webpages we will have to iterate over to get them all. \n",
    "\n",
    "The function, therefore, will begin by creating a BeautifulSoup object (i.e. a parsed HTML file) of the search url by opening the search url with urllib. Right away, we extract the total number of films returned from this query using the find_all method in BeautifulSoup, replacing the comma with nothing, and converting the resulting string to an integer. We then print the number for the users reference. From here, we need to understand how to move through each page of the query. By looking at page two, we see that it is, fortunately, pretty simple. By just appending \"&start=n\" where n is some number, to the search url, we can look at a page with 50 entries, starting at the nth entry. Getting a list of these numbers is a simple list comprehension. \n",
    "\n",
    "From there, we iterate each of these urls, creating a new BeautifulSoup object each time, and grabbing all links from the page using the get and findall methods. From the list of links, we look for a specific string 'title/tt' that corresponds to movie webpages, add it to the list of final urls, checking to make sure it is not already there. Once it iterates through each page and adds all urls, a test is run to ensure that the number of movies in the search matches the length of the final url list. \n",
    "\n",
    "There was an issue initially with the length of the list exceeding the number of movies in the search, so some debugging was required.  In some cases, there was only one extra link, while in others there was more than 100. The webbrowser library was used to open every 50 links to see which page was adding extra information. In hindsight, a more efficient option may be to just print how many items were added to the list in each loop and look for values over 50. Regardless, the issue was that some movies had a link to sequels/prequels included with them. This did not show up on the webpage, but was included in the html file. Further, each valid url was listed 3 seperate times, one with an extra directory in the url. Since these were the only title links with a length of 4 when split on '/', that is ultimately how we filter out for valid urls. It also avoids checking for duplicate values in the list twice for a better optimized runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_extractor(search_url):\n",
    "    #setting up initial BeautifulSoup object from websearch \n",
    "    init_resp = urllib.request.urlopen(search_url)\n",
    "    init_soup = BeautifulSoup(init_resp, 'html.parser')\n",
    "    #extract the number of films that the query returned. Used to confirm at end and generate each url \n",
    "    number_of_films = int(str(init_soup.find_all('div', class_='desc')[0].find_all('span')[0]).split(' ')[2].replace(',',''))\n",
    "    print(number_of_films)\n",
    "    #each page has 50 movies, so setting up a list to iterate through the pages, set up blank list to store final urls\n",
    "    iterative_urls = [i for i in range(1,number_of_films, 50)]\n",
    "    url_list = []\n",
    "    #loop through the 50-spaced interger values to generate entire list of needed search urls \n",
    "    for i in iterative_urls:\n",
    "        # set url\n",
    "        url = search_url + '&start=' + str(i)\n",
    "        #set up the BeautifulSoup object for this specific page of the search\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(resp, 'html.parser')\n",
    "        #generating list of all links on this page\n",
    "        links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "        # printing out where we are in the query to monitor efficiency\n",
    "        print('Running query from {} to {}'.format(i, i+49))\n",
    "        # checking each link in each search page for title/tt keyword\n",
    "        for link in links:\n",
    "            if 'title/tt' in link:\n",
    "                #when the length is 4 of the split title url, that means it is part of query \n",
    "                #when the length is 3, it means that the movie is ancillary to the actual search (prequel/sequel)\n",
    "                if len(link.split('/')) == 4:\n",
    "                # format the resulting url in the correct manner and appending it to final list \n",
    "                    title_link = 'https://www.imdb.com' + '/' + link.split('/')[1] + '/' + link.split('/')[2] + '/?ref_=adv_li_i'\n",
    "                    if not title_link in url_list:\n",
    "                        url_list.append(title_link)\n",
    "                    else: \n",
    "                        continue \n",
    "                else: \n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    # Final test to make sure that the length of query equals the length of returned list and returning the final list\n",
    "    if len(url_list) == number_of_films:\n",
    "        print('All urls have been extracted successfully')\n",
    "    else: \n",
    "        print('WARNING: The number of films in this query was {}, but {} urls were returned'.format(number_of_films, len(url_list)))\n",
    "    return(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test = url_extractor('https://www.imdb.com/search/title/?title_type=feature&release_date=2018-01-01,2018-03-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Web Scraper\n",
    "\n",
    "In this part of the project we use web scraping to gather data about all of the query result movies from IMDB's web page in a format that will be usable for analysis. In particular, we want to collect information about movie title, movie's genres, country, language, filming location, production company, runtime, budget information, release date, directors, writers, stars acting in the movie, rating, review count and finally mpaa ratings.\n",
    "\n",
    "Since our goal is to scrape information from multiple webpages with the same html structure, first we had to understand the websites structure. To do this, we used Chrome's Developer tools to inspect the structure of only one page. By right clicking and hitting inspect you can see the HTML line that corresponds to the part of webpage you clicked at. There are lots of HTML lines nested within each tag and for each feature we found unique identifier of information we wanted to extract. Then, in order to get the content of the webpage, we downloaded the pages that we wanted to scrape by using the request library. The library makes a get request to IMDB's server and downloads the HTML contents of a given web page. After we run the request using requests.get method, we get back a response object that has a status_code, which equals to 200 if the page gets downloaded successfully. This enabled us to include error catching into our code and return a warning whenever web pages did not get downloaded successfully (status_code does not equal 200).  Next step is to extract relevant text from the downloaded HTML documents. We used BeautifulSoup library for to parse these documents. In order to do this, we first created an instance of the BeautifulSoup class, which gives us a BeautifulSoup object. BeautifulSoup object has a nested data structure. The tag objects within this nested structure allowed us to extract the relevant information. We used find_all method to find all instances of a tag in a webpage. Find_all returns a list, therefore we used list indexing to extract relevant text by using text method. We also used find_all method to extract by class, for example in line 39 we search for any div that has class being 'see-more inline canwrap'. Once we are able to uniquely identify how to get each piece of information:\n",
    "\n",
    "* we can loop through multiple urls\n",
    "* place get requests within the loop for each page\n",
    "* convert the response's html content to beautiful soup object\n",
    "* extract all containers from this object by using find_all method, if there is no information than continue to the next variable\n",
    "* use list indexing to access information\n",
    "* and save this information into pandasâ€™ data frame.\n",
    "\n",
    "At the end of this process, we have a data frame with each row representing information on each movie scraped and each column corresponding to the variable we scraped. \n",
    "\n",
    "One of the issues we had to deal with was not having all of the features present on each web page. For example, not all movies have information about genres, language, writers and etc. Therefore, we used try and except statements to scrape information when present and continue running the code when the information was missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(urls):\n",
    "    '''Scrape information about movies from imdbs website\n",
    "\n",
    "    Input\n",
    "    =====\n",
    "    urls: list of urls\n",
    "    \n",
    "    Output\n",
    "    ======\n",
    "    movies: dataframe containning as rows movies we scraped and columns the features scraped\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create a dataframe to store scraped data in (features as columns)\n",
    "    features = ['genre_0','genre_1','genre_2', 'genre_3','country','language','filming_locs','production_co','runtime',\n",
    "    'budget','gross_usa','release','director_0','director_1','rating','star_0','star_1','star_2','star_3','writer_0',\n",
    "    'writer_1','writer_2','review_count','title','open_week','cumulative', 'mpaa_rating']\n",
    "    \n",
    "    #number of rows of data frame = # of movies scraped, # of columns = # of features\n",
    "    movies = pd.DataFrame(data = np.empty((len(urls), len(features))),\n",
    "                         columns= features)\n",
    "    \n",
    "    #data set is initialized as nan\n",
    "    movies[:] = np.nan\n",
    "    \n",
    "    #create a list of rating we want to collect\n",
    "    rating_list = ['   G', '   PG', 'PG-13', '   R', 'NC-17', 'Not Rated', 'Unrated', \n",
    "                   'TV-Y', 'TV-Y7', 'TV-G', 'TV-PG','TV-14', 'TV-MA']   \n",
    "    \n",
    "    #loop through urls and scrape relevant features\n",
    "    for idx,url in enumerate(urls):\n",
    "        \n",
    "        resp = requests.get(url)#request content of the webpage and store in resp\n",
    "\n",
    "        #Pause the loop\n",
    "        sleep(randint(8,15))\n",
    "\n",
    "        #Error if  status codes is not 200\n",
    "        if resp.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(request, response.status_code))\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')#use pythons built in html parser\n",
    "        \n",
    "        #collect mpaa ratings\n",
    "        mpaa = soup.find_all('div', class_ = 'title_wrapper')[0].find_all('div', class_ = 'subtext')[0].text\n",
    "        for i in rating_list:\n",
    "            if i in mpaa:\n",
    "                movies.loc[idx, 'mpaa_rating'] = i\n",
    "                \n",
    "        #get all genres\n",
    "        genres_spec = soup.find_all('div', class_ = 'see-more inline canwrap')\n",
    "        for i in range(len(genres_spec)):\n",
    "            if 'Genres' in genres_spec[i].find('h4', class_ = 'inline').text:\n",
    "                genres = soup.find_all('div', class_ = 'see-more inline canwrap')[i].find_all('a')\n",
    "                movies.loc[idx,'genre_0'] = genres[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'genre_1']= genres[1].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'genre_2'] = genres[2].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'genre_3'] = genres[3].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        #Get country, language, filming location, production co, budget info variables\n",
    "        other_specs = soup.find_all('div', class_ = 'txt-block')\n",
    "        for i in range(len(other_specs)):\n",
    "            if other_specs[i].find('h4', class_ = 'inline') is None:\n",
    "                pass\n",
    "            elif 'Country' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'country'] = other_specs[i].a.text\n",
    "            elif 'Language' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'language'] = other_specs[i].a.text\n",
    "            elif 'Filming Locations' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'filming_locs'] = other_specs[i].a.text\n",
    "            elif 'Production Co' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'production_co'] =other_specs[i].a.text\n",
    "            elif 'Runtime' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx, 'runtime'] = other_specs[i].time.text\n",
    "            elif 'Budget' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'budget']= other_specs[i].text\n",
    "                except: \n",
    "                    pass\n",
    "            elif 'Gross USA' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'gross_usa'] = other_specs[i].text\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'Opening Weekend USA' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'open_week'] = other_specs[i].text\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'Cumulative Worldwide Gross' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'cumulative'] = other_specs[i].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        #get release date\n",
    "        try:\n",
    "            movies.loc[idx,'release'] = soup.find_all('div', class_ = 'subtext')[0].find_all('a', title = \"See more release dates\")[0].text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #Get director, writer and stars    \n",
    "        movie_containers = soup.find_all('div', class_ = 'credit_summary_item')\n",
    "        for i in range(len(movie_containers)):\n",
    "            if 'Director' in movie_containers[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'director_0'] = movie_containers[i].find_all('a')[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'director_1']= movie_containers[i].find_all('a')[1].text\n",
    "                except:\n",
    "                    pass\n",
    "            if 'Writer' in movie_containers[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'writer_0']=movie_containers[i].find_all('a')[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'writer_1']=movie_containers[i].find_all('a')[1].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'writer_2']=movie_containers[i].find_all('a')[2].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if 'Star' in movie_containers[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'star_0']=movie_containers[i].find_all('a')[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'star_1']=movie_containers[i].find_all('a')[1].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'star_2']=movie_containers[i].find_all('a')[2].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'star_3']=movie_containers[i].find_all('a')[3].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        #Get raitng, review count and the title of the movie\n",
    "        try:\n",
    "            movies.loc[idx,'rating']=soup.find_all('span',{'itemprop':'ratingValue'})[0].text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            movies.loc[idx,'review_count']=soup.find_all('span',{'itemprop':'ratingCount'})[0].text\n",
    "        except:\n",
    "            pass\n",
    "        movies.loc[idx,'title']=soup.find_all('title')[0].text\n",
    "        \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning the Data\n",
    "\n",
    "The final step of the project is to clean the data set and prepare it for data anlaysis. When the data is read in from the scrapper, beautiful soup captures other elements the html code such as line breaks and tab characters that restricts our ability to gain any predictive insights from the data. Our goal in the cleaning of the data is three fold: All columns in the data frame should be of the appropriate data type will no 'bad characters', all place holders/category extenders should be removed and categorical features encoded when necessary. To do so we break the cleaning process into three functions: clean_data, encode_genre and adjust_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    num_cols = ['runtime', 'budget', 'gross_usa', 'rating', 'review_count', 'open_week', 'cumulative']\n",
    "    output = df.copy()\n",
    "    \n",
    "    # removing all strings from numeric columns \n",
    "    for col in num_cols:\n",
    "        output[col] = (output[col].str.replace('\\D+', '')).astype(\"float64\")\n",
    "        \n",
    "    # cleaning null/place holder values   \n",
    "    output = output.drop(['filming_locs'], axis = 1)\n",
    "    output = output.fillna(0)\n",
    "    output.replace(to_replace = \"See full cast & crew\", value = \"\", inplace = True)\n",
    "    output.replace(to_replace = \"[0-9][0-9]* more credits*\", value = \"\", inplace = True, regex=True)\n",
    "    \n",
    "    # removing 'bad' string from title and date (still need to fix datetime)\n",
    "    output['title'] = output['title'].str[0:-14]\n",
    "    output['release'] = output['release'].str[:-7]\n",
    "    \n",
    "    result = output.loc[(output[['budget', 'gross_usa', 'open_week', 'cumulative']] != 0).any(axis = 1)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_genre(df):\n",
    "    # encoding genre as dummie variables\n",
    "    output = pd.get_dummies(df, prefix_sep='_', columns = ['genre_0', 'genre_1', 'genre_2', 'genre_3'], drop_first=True)\n",
    "\n",
    "    #retreives all rows with 'genre'\n",
    "    genre_ls_all = output.filter(like='genre_').columns.tolist()\n",
    "\n",
    "    # aggregates genre buckets into single bucket\n",
    "    genre_ls = []\n",
    "    for item in genre_ls_all:\n",
    "        if item[9:] not in genre_ls:\n",
    "            genre_ls.append(item[9:])\n",
    "\n",
    "    for col in genre_ls:\n",
    "        output[col] = output.filter(like = col).sum(axis = 1)\n",
    "\n",
    "    output = output[output.columns.drop(list(output.filter(regex='genre')))]\n",
    "#     output['Music'].replace(2, 1, inplace=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_years(df):\n",
    "    output = df.release.str.split(' ', expand = True)\n",
    "    output.columns = ['release_day', 'release_month', 'release_year']\n",
    "    result = pd.concat([output.drop(['release_day'], axis = 1), df.drop(['release'], axis = 1)], axis = 1)\n",
    "    result['release_year'] = result.release_year.astype('int')\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
