{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Web Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "William Egan - wve204 | Tinatin Nikvashvili - tn709 | Nathan Griffin - nlg297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "IMDB offers downloadable datasets, but all of these are limited in scope. https://www.imdb.com/interfaces/ None contain any more than 8 features, and it would be complicated data munging task in and off itself to merge all of these into a single data frame. There are also a limited number of features offered by these, including financial information about the film, which is a feature of great interest for most data-driven business problems relating to film.\n",
    "IMDB remains a rich source of information, but this limited access to it inspired us to employ our python skills to develop a web scraper for pulling exactly the information we want from webpages for films. Along with determining all the features we want,  we also wanted to build a data set based on parameters of our choice, such as genre or year of release. A program that can effectively pull data sets of movies with any desired features or movie type would be useful for anyone who wants to do any kind of data analysis on films. The only limit is what kind of project the user can dream up. \n",
    "Another facet of our inspiration for this project is our dissatisfaction with the system of critic reviews and ratings. These metrics are biased and inconsistent, both across the spectrum of different critics and even within individual criticsâ€™ histories of publication. We will propose a new, data-driven method based on the data sets we can scrape from IMDB, and discuss the kinds of predictions we hope to be able to perform with this information. (we will address this after the description of our application)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract urls from IMBD search\n",
    "[IMDB Search](https://www.imdb.com/search/title/)\n",
    "\n",
    "This project is motivated by the groups interest in movies and being able to pull some specific datasets for analysis on demand. While IMDB does have some datasets puiblicly available, they are small (at most 8 columns), and would require significant merging to obtain what we are going for. Further, there is no box office information in these datasets, and earnings is the most significant variable that we would like to analyze. Beyond insterest in this industry, this application is also motivated by the fact that web scraping is a valuable tool for every data scientist to have at their disposal. Any time there is a lack of data in a problem you are facing, there is potentially valuable information available somewhere on the internet, and being able to extract it, clean it, and test it's validity allows for solving problems that otherwise may seem unsolvable. \n",
    "\n",
    "The first step in this application is to take an input search (link above) and return a list of urls: one for each movie that satisfies those parameters. To implement this function, we will need the help of a few libraries: BeautifulSoup, urllib, and webbrowser (for debugging). The first thing to do is just take a look at what the search returns in the actual webpage. The things that jump out are: the total number of movies that match the search listed at the top as well as there being 50 movies per page. Therefore, we know two important variables right off the bat: how many urls we expect to return and how many webpages we will have to iterate over to get them all. \n",
    "\n",
    "The function, therefore, will begin by creating a BeautifulSoup object (i.e. a parsed HTML file) of the search url by opening the search url with urllib. Right away, we extract the total number of films returned from this query using the find_all method in BeautifulSoup, replacing the comma with nothing, and converting the resulting string to an integer. We then print the number for the users reference. From here, we need to understand how to move through each page of the query. By looking at page two, we see that it is, fortunately, pretty simple. By just appending \"&start=n\" where n is some number, to the search url, we can look at a page with 50 entries, starting at the nth entry. Getting a list of these numbers is a simple list comprehension. \n",
    "\n",
    "From there, we iterate each of these urls, creating a new BeautifulSoup object each time, and grabbing all links from the page using the get and findall methods. From the list of links, we look for a specific string 'title/tt' that corresponds to movie webpages, add it to the list of final urls, checking to make sure it is not already there. Once it iterates through each page and adds all urls, a test is run to ensure that the number of movies in the search matches the length of the final url list. \n",
    "\n",
    "There was an issue initially with the length of the list exceeding the number of movies in the search, so some debugging was required.  In some cases, there was only one extra link, while in others there was more than 100. The webbrowser library was used to open every 50 links to see which page was adding extra information. In hindsight, a more efficient option may be to just print how many items were added to the list in each loop and look for values over 50. Regardless, the issue was that some movies had a link to sequels/prequels included with them. This did not show up on the webpage, but was included in the html file. Further, each valid url was listed 3 seperate times, one with an extra directory in the url. Since these were the only title links with a length of 4 when split on '/', that is ultimately how we filter out for valid urls. It also avoids checking for duplicate values in the list twice for a better optimized runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_extractor(search_url):\n",
    "    #setting up initial BeautifulSoup object from websearch \n",
    "    init_resp = urllib.request.urlopen(search_url)\n",
    "    init_soup = BeautifulSoup(init_resp, 'html.parser')\n",
    "    #extract the number of films that the query returned. Used to confirm at end and generate each url \n",
    "    number_of_films = int(str(init_soup.find_all('div', class_='desc')[0].find_all('span')[0]).split(' ')[2].replace(',',''))\n",
    "    print(number_of_films)\n",
    "    #each page has 50 movies, so setting up a list to iterate through the pages, set up blank list to store final urls\n",
    "    iterative_urls = [i for i in range(1,number_of_films, 50)]\n",
    "    url_list = []\n",
    "    #loop through the 50-spaced interger values to generate entire list of needed search urls \n",
    "    for i in iterative_urls:\n",
    "        # set url\n",
    "        url = search_url + '&start=' + str(i)\n",
    "        #set up the BeautifulSoup object for this specific page of the search\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(resp, 'html.parser')\n",
    "        #generating list of all links on this page\n",
    "        links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "        # printing out where we are in the query to monitor efficiency\n",
    "        print('Running query from {} to {}'.format(i, i+49))\n",
    "        # checking each link in each search page for title/tt keyword\n",
    "        for link in links:\n",
    "            if 'title/tt' in link:\n",
    "                #when the length is 4 of the split title url, that means it is part of query \n",
    "                #when the length is 3, it means that the movie is ancillary to the actual search (prequel/sequel)\n",
    "                if len(link.split('/')) == 4:\n",
    "                # format the resulting url in the correct manner and appending it to final list \n",
    "                    title_link = 'https://www.imdb.com' + '/' + link.split('/')[1] + '/' + link.split('/')[2] + '/?ref_=adv_li_i'\n",
    "                    if not title_link in url_list:\n",
    "                        url_list.append(title_link)\n",
    "                    else: \n",
    "                        continue \n",
    "                else: \n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    # Final test to make sure that the length of query equals the length of returned list and returning the final list\n",
    "    if len(url_list) == number_of_films:\n",
    "        print('All urls have been extracted successfully')\n",
    "    else: \n",
    "        print('WARNING: The number of films in this query was {}, but {} urls were returned'.format(number_of_films, len(url_list)))\n",
    "    return(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Web Scraper\n",
    "\n",
    "In this part of the project we use web scraping to gather data about all of the query result movies from IMDB's web page in a format that will be usable for analysis. In particular, we want to collect information about movie title, movie's genres, country, language, filming location, production company, runtime, budget information, release date, directors, writers, stars acting in the movie, rating, review count and finally mpaa ratings.\n",
    "\n",
    "Since our goal is to scrape information from multiple webpages with the same html structure, first we had to understand the websites structure. To do this, we used Chrome's Developer tools to inspect the structure of only one page. By right clicking and hitting inspect you can see the HTML line that corresponds to the part of webpage you clicked at. There are lots of HTML lines nested within each tag and for each feature we found unique identifier of information we wanted to extract. Then, in order to get the content of the webpage, we downloaded the pages that we wanted to scrape by using the request library. The library makes a get request to IMDB's server and downloads the HTML contents of a given web page. After we run the request using requests.get method, we get back a response object that has a status_code, which equals to 200 if the page gets downloaded successfully. This enabled us to include error catching into our code and return a warning whenever web pages did not get downloaded successfully (status_code does not equal 200).  Next step is to extract relevant text from the downloaded HTML documents. We used BeautifulSoup library for to parse these documents. In order to do this, we first created an instance of the BeautifulSoup class, which gives us a BeautifulSoup object. BeautifulSoup object has a nested data structure. The tag objects within this nested structure allowed us to extract the relevant information. We used find_all method to find all instances of a tag in a webpage. Find_all returns a list, therefore we used list indexing to extract relevant text by using text method. We also used find_all method to extract by class, for example in line 39 we search for any div that has class being 'see-more inline canwrap'. Once we are able to uniquely identify how to get each piece of information:\n",
    "\n",
    "* we can loop through multiple urls\n",
    "* place get requests within the loop for each page\n",
    "* convert the response's html content to beautiful soup object\n",
    "* extract all containers from this object by using find_all method, if there is no information than continue to the next variable\n",
    "* use list indexing to access information\n",
    "* and save this information into pandasâ€™ data frame.\n",
    "\n",
    "At the end of this process, we have a data frame with each row representing information on each movie scraped and each column corresponding to the variable we scraped. \n",
    "\n",
    "One of the issues we had to deal with was not having all of the features present on each web page. For example, not all movies have information about genres, language, writers and etc. Therefore, we used try and except statements to scrape information when present and continue running the code when the information was missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(urls):\n",
    "    '''Scrape information about movies from imdbs website\n",
    "\n",
    "    Input\n",
    "    =====\n",
    "    urls: list of urls\n",
    "    \n",
    "    Output\n",
    "    ======\n",
    "    movies: dataframe containning as rows movies we scraped and columns the features scraped\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create a dataframe to store scraped data in (features as columns)\n",
    "    features = ['genre_0','genre_1','genre_2', 'genre_3','country','language','filming_locs','production_co','runtime',\n",
    "    'budget','gross_usa','release','director_0','director_1','rating','star_0','star_1','star_2','star_3','writer_0',\n",
    "    'writer_1','writer_2','review_count','title','open_week','cumulative', 'mpaa_rating']\n",
    "    \n",
    "    #number of rows of data frame = # of movies scraped, # of columns = # of features\n",
    "    movies = pd.DataFrame(data = np.empty((len(urls), len(features))),\n",
    "                         columns= features)\n",
    "    \n",
    "    #data set is initialized as nan\n",
    "    movies[:] = np.nan\n",
    "    \n",
    "    #create a list of rating we want to collect\n",
    "    rating_list = ['   G', '   PG', 'PG-13', '   R', 'NC-17', 'Not Rated', 'Unrated', \n",
    "                   'TV-Y', 'TV-Y7', 'TV-G', 'TV-PG','TV-14', 'TV-MA']   \n",
    "    \n",
    "    count = 1\n",
    "    #loop through urls and scrape relevant features\n",
    "    for idx,url in enumerate(urls):\n",
    "        count += 1\n",
    "        \n",
    "        resp = requests.get(url)#request content of the webpage and store in resp\n",
    "\n",
    "        #Pause the loop but takes a long time\n",
    "        #sleep(randint(8,15))\n",
    "\n",
    "        #Error if  status codes is not 200\n",
    "        if resp.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(request, response.status_code))\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')#use pythons built in html parser\n",
    "        \n",
    "        #collect mpaa ratings\n",
    "        mpaa = soup.find_all('div', class_ = 'title_wrapper')[0].find_all('div', class_ = 'subtext')[0].text\n",
    "        for i in rating_list:\n",
    "            if i in mpaa:\n",
    "                movies.loc[idx, 'mpaa_rating'] = i\n",
    "                \n",
    "        #get all genres\n",
    "        genres_spec = soup.find_all('div', class_ = 'see-more inline canwrap')\n",
    "        for i in range(len(genres_spec)):\n",
    "            if 'Genres' in genres_spec[i].find('h4', class_ = 'inline').text:\n",
    "                genres = soup.find_all('div', class_ = 'see-more inline canwrap')[i].find_all('a')\n",
    "                movies.loc[idx,'genre_0'] = genres[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'genre_1']= genres[1].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'genre_2'] = genres[2].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'genre_3'] = genres[3].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        #Get country, language, filming location, production co, budget info variables\n",
    "        other_specs = soup.find_all('div', class_ = 'txt-block')\n",
    "        for i in range(len(other_specs)):\n",
    "            if other_specs[i].find('h4', class_ = 'inline') is None:\n",
    "                pass\n",
    "            elif 'Country' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'country'] = other_specs[i].a.text\n",
    "            elif 'Language' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'language'] = other_specs[i].a.text\n",
    "            elif 'Filming Locations' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'filming_locs'] = other_specs[i].a.text\n",
    "            elif 'Production Co' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'production_co'] =other_specs[i].a.text\n",
    "            elif 'Runtime' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx, 'runtime'] = other_specs[i].time.text\n",
    "            elif 'Budget' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'budget']= other_specs[i].text\n",
    "                except: \n",
    "                    pass\n",
    "            elif 'Gross USA' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'gross_usa'] = other_specs[i].text\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'Opening Weekend USA' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'open_week'] = other_specs[i].text\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'Cumulative Worldwide Gross' in other_specs[i].find('h4', class_ = 'inline').text:\n",
    "                try:\n",
    "                    movies.loc[idx,'cumulative'] = other_specs[i].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        #get release date\n",
    "        try:\n",
    "            movies.loc[idx,'release'] = soup.find_all('div', class_ = 'subtext')[0].find_all('a', title = \"See more release dates\")[0].text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #Get director, writer and stars    \n",
    "        movie_containers = soup.find_all('div', class_ = 'credit_summary_item')\n",
    "        for i in range(len(movie_containers)):\n",
    "            if 'Director' in movie_containers[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'director_0'] = movie_containers[i].find_all('a')[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'director_1']= movie_containers[i].find_all('a')[1].text\n",
    "                except:\n",
    "                    pass\n",
    "            if 'Writer' in movie_containers[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'writer_0']=movie_containers[i].find_all('a')[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'writer_1']=movie_containers[i].find_all('a')[1].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'writer_2']=movie_containers[i].find_all('a')[2].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if 'Star' in movie_containers[i].find('h4', class_ = 'inline').text:\n",
    "                movies.loc[idx,'star_0']=movie_containers[i].find_all('a')[0].text\n",
    "                try:\n",
    "                    movies.loc[idx,'star_1']=movie_containers[i].find_all('a')[1].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'star_2']=movie_containers[i].find_all('a')[2].text\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    movies.loc[idx,'star_3']=movie_containers[i].find_all('a')[3].text\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        #Get raitng, review count and the title of the movie\n",
    "        try:\n",
    "            movies.loc[idx,'rating']=soup.find_all('span',{'itemprop':'ratingValue'})[0].text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            movies.loc[idx,'review_count']=soup.find_all('span',{'itemprop':'ratingCount'})[0].text\n",
    "        except:\n",
    "            pass\n",
    "        movies.loc[idx,'title']=soup.find_all('title')[0].text\n",
    "        \n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print('Extracted {}'.format(count)) \n",
    "        \n",
    "        \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning the Data\n",
    "\n",
    "The final step of the project is to clean the data set and prepare it for data anlaysis. When the data is read in from the scraper, beautiful soup captures other elements the html code such as line breaks and tab characters that restricts our ability to gain any predictive insights from the data. Our goal in the cleaning of the data is three fold: All columns in the data frame should be of the appropriate data type will no 'bad characters', all place holders/category extenders should be removed and categorical features encoded when necessary. To do so we break the cleaning process into three functions: \n",
    "\n",
    "* clean_data \n",
    "* encode_genre\n",
    "* adjust_years\n",
    "\n",
    "The clean_data has three primary functionality:\n",
    "* The first is to remove all bad characters from columns. Numerical information to be cleaned is the runtime, budget, gross usa, rating out of 10, number of reviews, opening week sales and cululative sales. All the numerical columns are cleaned and cast as float. Title and release date are also cleaned to remove bad characters. \n",
    "* The second is to clean place holder and Nans. Place holder categorical variables are replaces with an empty string and Nans and filled with zero. \n",
    "* For the the purpose of this analysis, we drop rows that do not have financial data (open_week, cumulative, etc.) since these are key features in our analysis later\n",
    "\n",
    "The categorical feature we chose to encode is genre. This is done using the encode_genre function. Based on IMDBs website description, there are a finite and countable number of genres to encode. For actors, writers and directors, we realized that there is the potential for an extremely large number of feature encoding since these features are mostly unique. Here we use the get_dummies function in pandas in conjunction with aggregating of genere to create a binary column for each genere label.\n",
    "\n",
    "The last step in the process is the adjust_years funtion. For the purpose of our analysis we only want the years the movie was released. It is important to note that this procedure can be extended to involve months if there is analysis that requires this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    num_cols = ['runtime', 'budget', 'gross_usa', 'rating', 'review_count', 'open_week', 'cumulative']\n",
    "    output = df.copy()\n",
    "    \n",
    "    # removing all strings from numeric columns \n",
    "    for col in num_cols:\n",
    "        output[col] = (output[col].str.replace('\\D+', '')).astype(\"float64\")\n",
    "        \n",
    "    # cleaning null/place holder values   \n",
    "    output = output.drop(['filming_locs'], axis = 1)\n",
    "    output = output.fillna(0)\n",
    "    output.replace(to_replace = \"See full cast & crew\", value = \"\", inplace = True)\n",
    "    output.replace(to_replace = \"[0-9][0-9]* more credits*\", value = \"\", inplace = True, regex=True)\n",
    "    \n",
    "    # removing 'bad' string from title and date (still need to fix datetime)\n",
    "    output['title'] = output['title'].str[0:-14]\n",
    "    output['release']\n",
    "    \n",
    "    result = output.loc[(output[['budget', 'gross_usa', 'open_week', 'cumulative']] != 0).any(axis = 1)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_genre(df):\n",
    "    # encoding genre as dummie variables\n",
    "    output = pd.get_dummies(df, prefix_sep='_', columns = ['genre_0', 'genre_1', 'genre_2', 'genre_3'], drop_first=True)\n",
    "\n",
    "    #retreives all columns with 'genre'\n",
    "    genre_ls_all = output.filter(like='genre_').columns.tolist()\n",
    "\n",
    "    # aggregates genre buckets into single bucket\n",
    "    genre_ls = []\n",
    "    for item in genre_ls_all:\n",
    "        if item[9:] not in genre_ls:\n",
    "            genre_ls.append(item[9:])\n",
    "\n",
    "    for col in genre_ls:\n",
    "        output[col] = output.filter(like = col).sum(axis = 1)\n",
    "        \n",
    "    for col in genre_ls:\n",
    "        output[col] = output[col].clip(upper = 1)\n",
    "\n",
    "    output = output[output.columns.drop(list(output.filter(regex='genre')))]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_years(df):\n",
    "    df = df.reset_index(drop = True)\n",
    "    year_df = df['release'].str.extract(r'(\\b\\d{4}\\b)')\n",
    "    year_df.columns = ['Year']\n",
    "    result = pd.concat([year_df, df.drop(['release'], axis = 1)], axis = 1)\n",
    "    result['Year'] = result.Year.astype('int')\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647\n",
      "Running query from 1 to 50\n",
      "Running query from 51 to 100\n",
      "Running query from 101 to 150\n",
      "Running query from 151 to 200\n",
      "Running query from 201 to 250\n",
      "Running query from 251 to 300\n",
      "Running query from 301 to 350\n",
      "Running query from 351 to 400\n",
      "Running query from 401 to 450\n",
      "Running query from 451 to 500\n",
      "Running query from 501 to 550\n",
      "Running query from 551 to 600\n",
      "Running query from 601 to 650\n",
      "Running query from 651 to 700\n",
      "Running query from 701 to 750\n",
      "Running query from 751 to 800\n",
      "Running query from 801 to 850\n",
      "Running query from 851 to 900\n",
      "Running query from 901 to 950\n",
      "Running query from 951 to 1000\n",
      "Running query from 1001 to 1050\n",
      "Running query from 1051 to 1100\n",
      "Running query from 1101 to 1150\n",
      "Running query from 1151 to 1200\n",
      "Running query from 1201 to 1250\n",
      "Running query from 1251 to 1300\n",
      "Running query from 1301 to 1350\n",
      "Running query from 1351 to 1400\n",
      "Running query from 1401 to 1450\n",
      "Running query from 1451 to 1500\n",
      "Running query from 1501 to 1550\n",
      "Running query from 1551 to 1600\n",
      "Running query from 1601 to 1650\n",
      "All urls have been extracted successfully\n"
     ]
    }
   ],
   "source": [
    "url = url_extractor('https://www.imdb.com/search/title/?title_type=feature&release_date=2018-01-01,2018-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100\n",
      "Extracted 200\n",
      "Extracted 300\n",
      "Extracted 400\n",
      "Extracted 500\n",
      "Extracted 600\n",
      "Extracted 700\n",
      "Extracted 800\n",
      "Extracted 900\n",
      "Extracted 1000\n",
      "Extracted 1100\n",
      "Extracted 1200\n",
      "Extracted 1300\n",
      "Extracted 1400\n",
      "Extracted 1500\n",
      "Extracted 1600\n"
     ]
    }
   ],
   "source": [
    "movies = scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>production_co</th>\n",
       "      <th>runtime</th>\n",
       "      <th>budget</th>\n",
       "      <th>gross_usa</th>\n",
       "      <th>director_0</th>\n",
       "      <th>director_1</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>Music</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Reality-TV</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Western</th>\n",
       "      <th>War</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>USA</td>\n",
       "      <td>English</td>\n",
       "      <td>Marvel Studios</td>\n",
       "      <td>134.0</td>\n",
       "      <td>200000000.0</td>\n",
       "      <td>700059566.0</td>\n",
       "      <td>Ryan Coogler</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>USA</td>\n",
       "      <td>English</td>\n",
       "      <td>A24</td>\n",
       "      <td>127.0</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>44069456.0</td>\n",
       "      <td>Ari Aster</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>USA</td>\n",
       "      <td>English</td>\n",
       "      <td>Chernin Entertainment</td>\n",
       "      <td>140.0</td>\n",
       "      <td>69000000.0</td>\n",
       "      <td>46874505.0</td>\n",
       "      <td>Francis Lawrence</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>USA</td>\n",
       "      <td>English</td>\n",
       "      <td>Alcon Entertainment</td>\n",
       "      <td>130.0</td>\n",
       "      <td>35000000.0</td>\n",
       "      <td>45819713.0</td>\n",
       "      <td>Nicolai Fuglsig</td>\n",
       "      <td>0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>USA</td>\n",
       "      <td>English</td>\n",
       "      <td>Atmosphere Entertainment MM</td>\n",
       "      <td>148.0</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>44947622.0</td>\n",
       "      <td>Christian Gudegast</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year country language                 production_co  runtime       budget  \\\n",
       "0  2018     USA  English                Marvel Studios    134.0  200000000.0   \n",
       "1  2018     USA  English                           A24    127.0   10000000.0   \n",
       "2  2018     USA  English         Chernin Entertainment    140.0   69000000.0   \n",
       "3  2018     USA  English           Alcon Entertainment    130.0   35000000.0   \n",
       "4  2018     USA  English   Atmosphere Entertainment MM    148.0   30000000.0   \n",
       "\n",
       "     gross_usa          director_0 director_1  rating  ... Music Musical  \\\n",
       "0  700059566.0        Ryan Coogler          0    73.0  ...     0       0   \n",
       "1   44069456.0           Ari Aster          0    73.0  ...     0       0   \n",
       "2   46874505.0    Francis Lawrence          0    66.0  ...     0       0   \n",
       "3   45819713.0     Nicolai Fuglsig          0    65.0  ...     0       0   \n",
       "4   44947622.0  Christian Gudegast          0    70.0  ...     0       0   \n",
       "\n",
       "  Mystery Reality-TV Romance Sci-Fi Sport  Thriller Western  War  \n",
       "0       0          0       0      1     0         0       0    0  \n",
       "1       1          0       0      0     0         1       0    0  \n",
       "2       0          0       0      0     0         1       0    0  \n",
       "3       0          0       0      0     0         0       0    1  \n",
       "4       1          0       0      0     0         0       0    0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_movies = clean_data(movies)\n",
    "encoded = encode_genre(clean_movies)\n",
    "df_final = adjust_years(encoded)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"df_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
